{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First-tier classifier to predict total cost class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import seaborn as sns\n",
    "\n",
    "import sklearn\n",
    "import scipy\n",
    "import scipy.stats as stats\n",
    "\n",
    "import missingno as msno\n",
    "import plotly.graph_objects as go\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn import base\n",
    "from collections import defaultdict\n",
    "from matplotlib.ticker import FixedLocator, FixedFormatter\n",
    "from joblib import dump, load\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, \\\n",
    "RandomizedSearchCV, cross_val_score, RepeatedStratifiedKFold, KFold\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from kmodes.kprototypes import KPrototypes\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"data.xlsx\",sheet_name='Sheet1', usecols=\"A:AP\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.drop(['Indirect_insurance_cost_category_code','Total_loss_cost'], axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# example of oversampling a multi-class classification dataset\n",
    "from pandas import read_csv\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "from matplotlib import pyplot\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import category_encoders as ce\n",
    "X = df.drop(columns=['Total_loss_cost_classification_code'])\n",
    "y = df['Total_loss_cost_classification_code']\n",
    "encoder = ce.OrdinalEncoder(cols=['Construction_type_classification_code','Accident_type_code', 'Work_process_classification_code','Injury_area_classification_code','Workers_affiliation','Integrated_occupation_classification_code','Direct_insurance_cost_category_code'])\n",
    "\n",
    "\n",
    "X = encoder.fit_transform(X)\n",
    "\n",
    "X = encoder.transform(X)\n",
    "y = LabelEncoder().fit_transform(y)\n",
    "# transform the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2) #2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import RocCurveDisplay, roc_curve\n",
    "def model_eval(clf, X_test=X_test, y_test=y_test):\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(\"Mean cross-validated score of the best_estimator: {0:.3f}\".format(clf.best_score_))\n",
    "    print(\"Accuracy on train data: {0:.3f}\".format(clf.score(X_train, y_train)))\n",
    "    print(\"Accuracy on test data: {0:.3f}\".format(clf.score(X_test, y_test)))\n",
    "    print(metrics.classification_report(y_test, y_pred))\n",
    "    print(\"Tuned Model Parameters: {}\".format(clf.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_list={}\n",
    "param_grid = dict(\n",
    "    criterion=['gini', 'entropy'],\n",
    "    min_samples_split=[2, 10],\n",
    "    max_depth=[5, 6, 7, 8, 9],\n",
    "    min_samples_leaf=[1, 10],\n",
    "    max_leaf_nodes=[10, 20])\n",
    "dt_gscv = GridSearchCV(DecisionTreeClassifier(random_state=0), \n",
    "                       param_grid, scoring='accuracy')\n",
    "dt_gscv.fit(X_train, y_train)\n",
    "model_eval(dt_gscv)\n",
    "result_list['DT'] = dt_gscv.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "param_grid = dict(\n",
    "    criterion=['gini', 'entropy'],\n",
    "    min_samples_split=[8, 16, 20],\n",
    "    max_depth=[6, 8, 10, 12],\n",
    "    min_samples_leaf=[8, 12, 18],\n",
    "    max_leaf_nodes=[10, 20],\n",
    "    n_estimators=[10,100,200])\n",
    "rt_gscv = GridSearchCV(RandomForestClassifier(random_state=0), \n",
    "                       param_grid, scoring='accuracy')\n",
    "rt_gscv.fit(X_train, y_train)\n",
    "model_eval(rt_gscv)\n",
    "result_list['RF'] = rt_gscv.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-nearest Neighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_clf = KNeighborsClassifier()\n",
    "param_grid = {\n",
    "    'n_neighbors': [2,3, 5, 7, 9, 12, 15],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan']\n",
    "}\n",
    "knn_gscv = GridSearchCV(knn_clf, param_grid=param_grid)\n",
    "knn_gscv.fit(X_train, y_train)\n",
    "model_eval(knn_gscv)\n",
    "result_list['KNN'] = knn_gscv.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "params = {\n",
    "            'objective':'reg:logistic',\n",
    "            'max_depth': 4,\n",
    "            'alpha': 10,\n",
    "            'learning_rate': 0.01,\n",
    "            'n_estimators':100\n",
    "        }\n",
    "            \n",
    "            \n",
    "            \n",
    "# instantiate the classifier \n",
    "xgb_clf = XGBClassifier(**params,enable_categorical=True)\n",
    "\n",
    "\n",
    "\n",
    "# fit the classifier to the training data\n",
    "xgb_clf.fit(X_train, y_train)\n",
    "y_pred = xgb_clf.predict(X_test)\n",
    "# check accuracy score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print('XGBoost model accuracy score: {0:0.4f}'. format(accuracy_score(y_test, y_pred)))\n",
    "print(metrics.classification_report(y_test, y_pred))\n",
    "print(\"Accuracy on train data: {0:.3f}\".format(xgb_clf.score(X_train, y_train)))\n",
    "result_list['XGBoost'] = xgb_clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Undersampling of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "rus = RandomUnderSampler(sampling_strategy='majority')\n",
    "rus.fit(X, y)\n",
    "X_resampled1, y_resampled1 = rus.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled1, y_resampled1, random_state=42, test_size=0.2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import RocCurveDisplay, roc_curve\n",
    "def model_eval(clf, X_test=X_test, y_test=y_test):\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(\"Mean cross-validated score of the best_estimator: {0:.3f}\".format(clf.best_score_))\n",
    "    print(\"Accuracy on train data: {0:.3f}\".format(clf.score(X_train, y_train)))\n",
    "    print(\"Accuracy on test data: {0:.3f}\".format(clf.score(X_test, y_test)))\n",
    "    print(metrics.classification_report(y_test, y_pred))\n",
    "    print(\"Tuned Model Parameters: {}\".format(clf.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_list={}\n",
    "param_grid = dict(\n",
    "    criterion=['gini', 'entropy'],\n",
    "    min_samples_split=[2, 10],\n",
    "    max_depth=[5, 6, 7, 8, 9],\n",
    "    min_samples_leaf=[1, 10],\n",
    "    max_leaf_nodes=[10, 20])\n",
    "dt_gscv = GridSearchCV(DecisionTreeClassifier(random_state=0), \n",
    "                       param_grid, scoring='accuracy')\n",
    "dt_gscv.fit(X_train, y_train)\n",
    "model_eval(dt_gscv)\n",
    "result_list['DT'] = dt_gscv.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "param_grid = dict(\n",
    "    criterion=['gini', 'entropy'],\n",
    "    min_samples_split=[2, 10],\n",
    "    max_depth=[5, 10],\n",
    "    min_samples_leaf=[1, 10],\n",
    "    max_leaf_nodes=[10, 20])\n",
    "rt_gscv = GridSearchCV(RandomForestClassifier(random_state=0), \n",
    "                       param_grid, scoring='accuracy')\n",
    "rt_gscv.fit(X_train, y_train)\n",
    "model_eval(rt_gscv)\n",
    "result_list['RF'] = rt_gscv.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_clf = KNeighborsClassifier()\n",
    "param_grid = {\n",
    "    'n_neighbors': [3, 5, 7, 9, 12, 15],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan']\n",
    "}\n",
    "knn_gscv = GridSearchCV(knn_clf, param_grid=param_grid)\n",
    "knn_gscv.fit(X_train, y_train)\n",
    "model_eval(knn_gscv)\n",
    "result_list['KNN'] = knn_gscv.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "params = {\n",
    "            'objective':'reg:logistic',\n",
    "            'max_depth': 4,\n",
    "            'alpha': 10,\n",
    "            'learning_rate': 1.0,\n",
    "            'n_estimators':100\n",
    "        }\n",
    "            \n",
    "            \n",
    "            \n",
    "# instantiate the classifier \n",
    "xgb_clf = XGBClassifier(**params,enable_categorical=True)\n",
    "\n",
    "\n",
    "\n",
    "# fit the classifier to the training data\n",
    "xgb_clf.fit(X_train, y_train)\n",
    "y_pred = xgb_clf.predict(X_test)\n",
    "# check accuracy score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print('XGBoost model accuracy score: {0:0.4f}'. format(accuracy_score(y_test, y_pred)))\n",
    "print(metrics.classification_report(y_test, y_pred))\n",
    "print(\"Accuracy on train data: {0:.3f}\".format(xgb_clf.score(X_train, y_train)))\n",
    "result_list['XGBoost'] = xgb_clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Oversampling of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "ros = RandomOverSampler(random_state=0)\n",
    "ros.fit(X, y)\n",
    "X_resampled, y_resampled = ros.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, random_state=42, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import RocCurveDisplay, roc_curve\n",
    "def model_eval(clf, X_test=X_test, y_test=y_test):\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(\"Mean cross-validated score of the best_estimator: {0:.3f}\".format(clf.best_score_))\n",
    "    print(\"Accuracy on train data: {0:.3f}\".format(clf.score(X_train, y_train)))\n",
    "    print(\"Accuracy on test data: {0:.3f}\".format(clf.score(X_test, y_test)))\n",
    "    print(metrics.classification_report(y_test, y_pred))\n",
    "    print(\"Tuned Model Parameters: {}\".format(clf.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_list={}\n",
    "param_grid = dict(\n",
    "    criterion=['gini', 'entropy'],\n",
    "    min_samples_split=[2, 10],\n",
    "    max_depth=[5, 6, 7, 8, 9],\n",
    "    min_samples_leaf=[1, 10],\n",
    "    max_leaf_nodes=[10, 20])\n",
    "dt_gscv = GridSearchCV(DecisionTreeClassifier(random_state=0), \n",
    "                       param_grid, scoring='accuracy')\n",
    "dt_gscv.fit(X_train, y_train)\n",
    "model_eval(dt_gscv)\n",
    "result_list['DT'] = dt_gscv.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "param_grid = dict(\n",
    "    criterion=['gini', 'entropy'],\n",
    "    min_samples_split=[2, 10],\n",
    "    max_depth=[5, 10],\n",
    "    min_samples_leaf=[1, 10],\n",
    "    max_leaf_nodes=[10, 20])\n",
    "rt_gscv = GridSearchCV(RandomForestClassifier(random_state=0), \n",
    "                       param_grid, scoring='accuracy')\n",
    "rt_gscv.fit(X_train, y_train)\n",
    "model_eval(rt_gscv)\n",
    "result_list['RF'] = rt_gscv.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_clf = KNeighborsClassifier()\n",
    "param_grid = {\n",
    "    'n_neighbors': [3, 5, 7, 9, 12, 15],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan']\n",
    "}\n",
    "knn_gscv = GridSearchCV(knn_clf, param_grid=param_grid)\n",
    "knn_gscv.fit(X_train, y_train)\n",
    "model_eval(knn_gscv)\n",
    "result_list['KNN'] = knn_gscv.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "params = {\n",
    "            'objective':'reg:logistic',\n",
    "            'max_depth': 4,\n",
    "            'alpha': 10,\n",
    "            'learning_rate': 1.0,\n",
    "            'n_estimators':100\n",
    "        }\n",
    "            \n",
    "            \n",
    "            \n",
    "# instantiate the classifier \n",
    "xgb_clf = XGBClassifier(**params,enable_categorical=True)\n",
    "\n",
    "\n",
    "\n",
    "# fit the classifier to the training data\n",
    "xgb_clf.fit(X_train, y_train)\n",
    "y_pred = xgb_clf.predict(X_test)\n",
    "# check accuracy score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print('XGBoost model accuracy score: {0:0.4f}'. format(accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second-tier regressor to predict indirect cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One hot encoding\n",
    "categorical_cols = ['Direct_insurance_cost_total_category_code','Total_loss_cost_classification_code']\n",
    "numerical_cols = ['Human_damage_death','Human_damage_injuries','Direct_insurance_costs']\n",
    "target_col = 'Indirect_insurance_costs'\n",
    "X = df[categorical_cols + numerical_cols]\n",
    "y = df[target_col]\n",
    "from collections import Counter\n",
    "from matplotlib import pyplot\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import category_encoders as ce\n",
    "\n",
    "encoder = ce.OrdinalEncoder(cols=['Total_loss_cost_classification_code','Direct_insurance_cost_total_category_code'])\n",
    "X = encoder.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test=train_test_split(X,y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler \n",
    "ss_X = StandardScaler()\n",
    "ss_y = StandardScaler()\n",
    "\n",
    "X_train = ss_X.fit_transform(X_train)\n",
    "X_test = ss_X.fit_transform(X_test)\n",
    "y_train = ss_y.fit_transform(y_train.values.reshape(-1,1))\n",
    "y_test = ss_y.fit_transform(y_test.values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dt = DecisionTreeRegressor()\n",
    "dt.fit(X_train, y_train)\n",
    "yhat = dt.predict(X_test)\n",
    "print(r2_score(y_test, yhat), mean_absolute_error(y_test, yhat), np.sqrt(mean_squared_error(y_test, yhat)))\n",
    "yhat=yhat.reshape(-1, 1)\n",
    "y1=ss_y.inverse_transform(yhat)\n",
    "y_test=y_test.reshape(-1, 1)\n",
    "y2=ss_y.inverse_transform(y_test)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Decalring numpy array variable\n",
    "\n",
    "xAxis = np.arange(y2.shape[0])\n",
    "\n",
    "plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "plt.rcParams.update({'font.size': 15})\n",
    "plt.ticklabel_format(style='plain', axis='y')\n",
    "plt.title(\"Decision Tree Regressor Result\")\n",
    "plt.xlabel(\"Test sample\")\n",
    "plt.ylabel(\"Indirect cost (US$)\")\n",
    "plt.plot(xAxis, y1, color =\"red\")\n",
    "plt.plot(xAxis, y2, color =\"black\",linestyle='dotted')\n",
    "plt.legend(['prediction', 'actual']);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import lightgbm as lgbm\n",
    "import xgboost as xg\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "\n",
    "regr_trans = TransformedTargetRegressor(regressor=lgbm.LGBMRegressor(), transformer=QuantileTransformer(output_distribution='normal'))\n",
    "regr_trans.fit(X_train, y_train)\n",
    "yhat = regr_trans.predict(X_test)\n",
    "print(round(r2_score(y_test, yhat),3), round(mean_absolute_error(y_test, yhat),2), round(np.sqrt(mean_squared_error(y_test, yhat)),2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat=yhat.reshape(-1, 1)\n",
    "y1=ss_y.inverse_transform(yhat)\n",
    "y_test=y_test.reshape(-1, 1)\n",
    "y2=ss_y.inverse_transform(y_test)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Decalring numpy array variable\n",
    "\n",
    "xAxis = np.arange(y2.shape[0])\n",
    "plt.ticklabel_format(style='plain', axis='y')\n",
    "plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "plt.rcParams.update({'font.size': 15})\n",
    "# plotting\n",
    "plt.title(\"LGBM Regressor Result\")\n",
    "plt.xlabel(\"Test sample\")\n",
    "plt.ylabel(\"Indirect cost (US$)\")\n",
    "plt.plot(xAxis, y1, color =\"red\")\n",
    "plt.plot(xAxis, y2, color =\"black\",linestyle='dotted')\n",
    "plt.legend(['prediction', 'actual']);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import lightgbm as lgbm\n",
    "import xgboost as xg\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "regr_trans = TransformedTargetRegressor(regressor=GradientBoostingRegressor(), transformer=QuantileTransformer(output_distribution='normal'))\n",
    "regr_trans.fit(X_train, y_train)\n",
    "yhat = regr_trans.predict(X_test)\n",
    "print(round(r2_score(y_test, yhat),3), round(mean_absolute_error(y_test, yhat),2), round(np.sqrt(mean_squared_error(y_test, yhat)),2))\n",
    "yhat=yhat.reshape(-1, 1)\n",
    "y1=ss_y.inverse_transform(yhat)\n",
    "y_test=y_test.reshape(-1, 1)\n",
    "y2=ss_y.inverse_transform(y_test)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Decalring numpy array variable\n",
    "\n",
    "xAxis = np.arange(y2.shape[0])\n",
    "plt.ticklabel_format(style='plain', axis='y')\n",
    "plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "plt.rcParams.update({'font.size': 15})\n",
    "# plotting\n",
    "plt.title(\"Gradient Boosting Regressor Result\")\n",
    "plt.xlabel(\"Test sample\")\n",
    "plt.ylabel(\"Indirect cost (US$)\")\n",
    "plt.plot(xAxis, y1, color =\"red\")\n",
    "plt.plot(xAxis, y2, color =\"black\",linestyle='dotted')\n",
    "plt.legend(['prediction', 'actual']);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "## Define Grid \n",
    "model = RandomForestRegressor()\n",
    "#transforming target variable through quantile transformer\n",
    "ttr = TransformedTargetRegressor(regressor=model, transformer=QuantileTransformer(output_distribution='normal'))\n",
    "ttr.fit(X_train, y_train)\n",
    "yhat = ttr.predict(X_test)\n",
    "print(r2_score(y_test, yhat), mean_absolute_error(y_test, yhat), np.sqrt(mean_squared_error(y_test, yhat)))\n",
    "yhat=yhat.reshape(-1, 1)\n",
    "y1=ss_y.inverse_transform(yhat)\n",
    "y_test=y_test.reshape(-1, 1)\n",
    "y2=ss_y.inverse_transform(y_test)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Decalring numpy array variable\n",
    "\n",
    "xAxis = np.arange(y2.shape[0])\n",
    "plt.ticklabel_format(style='plain', axis='y')\n",
    "plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "plt.rcParams.update({'font.size': 15})\n",
    "# plotting\n",
    "plt.title(\"Random Forest Regressor Result\")\n",
    "plt.xlabel(\"Test sample\")\n",
    "plt.ylabel(\"Indirect cost (US$)\")\n",
    "plt.plot(xAxis, y1, color =\"red\")\n",
    "plt.plot(xAxis, y2, color =\"black\",linestyle='dotted')\n",
    "plt.legend(['prediction', 'actual']);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
